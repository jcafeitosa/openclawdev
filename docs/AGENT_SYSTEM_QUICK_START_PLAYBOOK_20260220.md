# ğŸ® Agent System Quick-Start Playbook

_How to work with 100 agents effectively. Decision trees, playbooks, best practices._

---

## ğŸ¯ BEFORE YOU USE AN AGENT

### 1. Identify Your Need (30 seconds)

```
Do you need to...

â”œâ”€ Make a DECISION?
â”‚  â”œâ”€ Strategic ($100K+, org-wide) â†’ CEO/CTO
â”‚  â”œâ”€ Technical (architecture) â†’ CTO or Architect
â”‚  â”œâ”€ Product (feature priority) â†’ CPO/Product Manager
â”‚  â””â”€ Security (vulnerability) â†’ CISO/Security Engineer
â”‚
â”œâ”€ Execute WORK? (code, tests, analysis)
â”‚  â”œâ”€ API/Backend â†’ Carlos + engineers
â”‚  â”œâ”€ Frontend/UI â†’ Aninha + engineers
â”‚  â”œâ”€ Database â†’ Fernanda
â”‚  â”œâ”€ Security â†’ Mariana
â”‚  â”œâ”€ Performance â†’ Paulo
â”‚  â”œâ”€ Tests â†’ Tatiane/Samanta
â”‚  â””â”€ Analysis â†’ Sofia/Ricardo/Marcos
â”‚
â”œâ”€ Coordinate PEOPLE?
â”‚  â”œâ”€ Tactical (sprint planning) â†’ Product Manager
â”‚  â”œâ”€ Operations (team blockers) â†’ Engineering Manager
â”‚  â””â”€ Code quality (reviews) â†’ Tech Lead
â”‚
â””â”€ Review/Improve EXISTING?
   â”œâ”€ Code â†’ Matheus (Tech Lead) or Carlos
   â”œâ”€ Architecture â†’ Architects
   â”œâ”€ Performance â†’ Paulo
   â””â”€ Security â†’ Mariana
```

### 2. Check Agent Availability (10 seconds)

```bash
openclaw agents list
# Shows: status, current load, model assignment
```

### 3. Formulate Request (1-2 minutes)

```
Good: "Design API for user authentication with JWT"
Bad: "Make an API"

Good: "Optimize query that takes 5s on 100K rows"
Bad: "Make it faster"

Good: "Is this vulnerable to SQL injection? [code]"
Bad: "Is this secure?"
```

---

## ğŸš€ COMMON WORKFLOWS

### Workflow 1: DESIGN NEW FEATURE (End-to-End)

**Timeline**: 2-3 hours | **Team**: 4-5 agents | **Cost**: ~$10

```
Step 1: CLARIFY REQUIREMENTS (10 min)
  â”œâ”€ Ping: Product Manager (Bruno)
  â”œâ”€ Ask: "What are acceptance criteria for [feature]?"
  â””â”€ Receive: User stories + scope

Step 2: ARCHITECTURE DECISION (20 min)
  â”œâ”€ Ping: Backend Architect (Carlos)
  â”œâ”€ Ask: "API design for [feature]?"
  â”œâ”€ Ask: "Database schema implications?"
  â””â”€ Receive: API spec + schema design

Step 3: FRONTEND DESIGN (15 min)
  â”œâ”€ Ping: Frontend Architect (Aninha)
  â”œâ”€ Ask: "Component structure for [feature]?"
  â””â”€ Receive: Component hierarchy + state management

Step 4: IMPLEMENTATION PLANNING (15 min)
  â”œâ”€ Ping: Engineering Manager (Diego)
  â”œâ”€ Ask: "Break into tasks + assign engineers"
  â””â”€ Receive: Sprint tasks + owners

Step 5: QUALITY PLANNING (10 min)
  â”œâ”€ Ping: QA Lead (Isabela)
  â”œâ”€ Ask: "Test strategy for [feature]?"
  â””â”€ Receive: Test plan + acceptance criteria

Step 6: SECURITY REVIEW (10 min)
  â”œâ”€ Ping: Security Engineer (Mariana)
  â”œâ”€ Ask: "Any security concerns? [design]"
  â””â”€ Receive: Security sign-off or mitigations

Result: Feature is designed, ready to code
```

### Workflow 2: BUG DIAGNOSIS & FIX (1-2 hours)

**Timeline**: 1-2 hours | **Team**: 2-3 agents | **Cost**: ~$5

```
Step 1: REPORT ISSUE (5 min)
  â””â”€ "When I [action], [bad thing] happens. Expected: [good thing]"

Step 2: DIAGNOSE (20-30 min)
  â”œâ”€ Ping: Root Cause Analyst (Marcos)
  â”œâ”€ Provide: Error logs, code context, reproduction steps
  â””â”€ Receive: Root cause hypothesis + 3 possible fixes

Step 3: VALIDATE DIAGNOSIS (10 min)
  â”œâ”€ Ping: Relevant specialist (database, API, frontend)
  â””â”€ Receive: Confirmation + implementation guidance

Step 4: IMPLEMENT FIX (20-30 min)
  â”œâ”€ Ping: Engineer (or specialist)
  â”œâ”€ Provide: Diagnosis + architecture approval
  â””â”€ Receive: PR with fix + tests

Step 5: REVIEW & MERGE (10 min)
  â”œâ”€ Ping: Tech Lead (Matheus)
  â””â”€ Receive: LGTM + merge

Result: Bug fixed, deployed, monitored
```

### Workflow 3: CODE REVIEW (30-45 min)

**Timeline**: 30-45 min | **Team**: 1-2 agents | **Cost**: ~$2

```
Step 1: SUBMIT PR (immediate)
  â””â”€ GitHub: Create PR with description

Step 2: ARCHITECTURE REVIEW (10-15 min)
  â”œâ”€ Ping: Relevant Architect (Carlos/Aninha/Rafael)
  â”œâ”€ Context: PR link, design decision
  â””â”€ Receive: Architecture approval or suggestions

Step 3: CODE QUALITY REVIEW (10-15 min)
  â”œâ”€ Ping: Tech Lead (Matheus)
  â”œâ”€ Context: PR link
  â””â”€ Receive: Code quality feedback (patterns, style)

Step 4: SECURITY REVIEW (5-10 min)
  â”œâ”€ Ping: Security Engineer (Mariana) â€” if security-related
  â””â”€ Receive: Security approval or issues

Step 5: TEST COVERAGE (5 min)
  â”œâ”€ Ping: Testing Specialist (Tatiane)
  â”œâ”€ Context: Code changes + test files
  â””â”€ Receive: Coverage feedback

Result: Approved PR ready to merge
```

### Workflow 4: PERFORMANCE INVESTIGATION (1-2 hours)

**Timeline**: 1-2 hours | **Team**: 2-3 agents | **Cost**: ~$5

```
Step 1: REPORT SLOWNESS (5 min)
  â”œâ”€ "Feature X takes [time], expected [time]"
  â”œâ”€ Provide: Metrics, user impact, frequency
  â””â”€ Prioritize: Is it critical or annoying?

Step 2: INVESTIGATE (30 min)
  â”œâ”€ Ping: Performance Engineer (Paulo)
  â”œâ”€ Provide: Code, logs, reproduction steps
  â””â”€ Receive: Bottleneck identified + optimization options

Step 3: SELECT OPTIMIZATION (10 min)
  â”œâ”€ Ping: Relevant specialist (database/API/frontend)
  â”œâ”€ Consider: Trade-offs (complexity vs speed)
  â””â”€ Decide: Which optimization to implement

Step 4: IMPLEMENT (30 min)
  â”œâ”€ Ping: Engineer
  â”œâ”€ Provide: Optimization approach
  â””â”€ Receive: PR with optimized code

Step 5: VALIDATE (10 min)
  â”œâ”€ Measure: New performance
  â”œâ”€ Verify: Improvement meets target
  â””â”€ Accept: Or iterate if needed

Result: Performance improved, monitored
```

---

## ğŸ“‹ DECISION TREES

### "I need code reviewed. Who do I ping?"

```
Is it ARCHITECTURAL change?
â”œâ”€ YES â†’ Backend Architect (Carlos) or Frontend Architect (Aninha)
â”œâ”€ NO â†’ Tech Lead (Matheus)

Does it involve SECURITY?
â”œâ”€ YES â†’ Security Engineer (Mariana) [in parallel]
â”œâ”€ NO â†’ [skip]

Does it need TEST review?
â”œâ”€ YES â†’ Testing Specialist (Tatiane)
â”œâ”€ NO â†’ [skip]

Timeline: 10-15 min per reviewer
```

### "Performance is bad. What to investigate?"

```
Is it API LATENCY?
â”œâ”€ YES â†’ Performance Engineer (Paulo) + Backend Architect (Carlos)
â””â”€ NO â†’ Continue

Is it DATABASE QUERY?
â”œâ”€ YES â†’ Database Engineer (Fernanda)
â””â”€ NO â†’ Continue

Is it FRONTEND rendering?
â”œâ”€ YES â†’ Frontend Architect (Aninha) + Performance Engineer
â””â”€ NO â†’ Continue

Is it NETWORK/CDN?
â”œâ”€ YES â†’ DevOps Engineer (Thiago)
â””â”€ NO â†’ Continue

Root Cause Analyst (Marcos) can help if unclear
Timeline: 30 min diagnosis
```

### "Should we use X technology?"

```
Is it a FRAMEWORK (Elysia, Astro, Drizzle)?
â”œâ”€ YES â†’ Specialist (Miguel, Beatriz, Aline)
â”œâ”€ NO â†’ Continue

Is it INFRASTRUCTURE (Docker, Kubernetes)?
â”œâ”€ YES â†’ DevOps Engineer (Thiago) + System Architect (Pedro)
â”œâ”€ NO â†’ Continue

Is it DATABASE?
â”œâ”€ YES â†’ Database Engineer (Fernanda)
â”œâ”€ NO â†’ Continue

Is it ARCHITECTURAL choice (monolith vs micro)?
â”œâ”€ YES â†’ CTO (Rodrigo) + System Architect (Pedro)
â”œâ”€ NO â†’ Continue

Is it PLATFORM selection (AWS vs GCP)?
â”œâ”€ YES â†’ CTO (Rodrigo) + Deep Research (Ricardo)
â””â”€ NO â†’ Continue

Default: Deep Research (Ricardo) for evaluation
Timeline: 25 min analysis
```

---

## âš¡ SPEED TIPS

### 1. Parallel Requests (Save 50% Time)

```bash
# Instead of sequential:
1. Carlos: API design (20 min)
2. Aninha: Frontend design (20 min)
3. Total: 40 min

# Do parallel:
1. Carlos + Aninha simultaneously (20 min)
2. Total: 20 min âœ…
```

### 2. Use Fast Agents for Execution

```bash
# SLOW (25+ min): Ricardo, Marcos, Architectects
# MEDIUM (15 min): Engineers
# FAST (5-10 min): Sofia, Tatiane, specialists

# For quick tasks (< 5 min), use FAST agents
# For strategic decisions (> 20 min), use SLOW agents
```

### 3. Pre-Fill Context

```
BEFORE: "Why is this slow?"
AFTER: "This [function] takes 5s on 100K rows. [profiling data]. Why?"

BEFORE: "Design API"
AFTER: "Design API for [feature] with [constraints] based on [existing patterns]"

More context = faster response = lower cost
```

### 4. Batch Similar Tasks

```
DON'T:
1. Sofia: Analyze metric A (10 min)
2. Sofia: Analyze metric B (10 min)
3. Total: 20 min

DO:
1. Sofia: Analyze metrics A, B, C (12 min)
2. Total: 12 min âœ…
```

---

## ğŸ’° COST OPTIMIZATION

### Tier-Based Assignment

```
$$$$ (Expensive - 10% of requests)
â””â”€ Strategic decisions (CEO, CTO, Architects)
   â””â”€ Use when: Uncertain, high impact, expensive to redo

$$$ (Medium - 30% of requests)
â””â”€ Engineering decisions (Tech Lead, Engineers, DB specialist)
   â””â”€ Use when: Complex technical problem, needs expertise

$$ (Cheap - 40% of requests)
â””â”€ Execution tasks (Sofia, Tatiane, specialists)
   â””â”€ Use when: Straightforward coding, testing, analysis

$ (Free - 20% of requests)
â””â”€ Fallback models (Llama, Mistral, Qwen)
   â””â”€ Use when: Non-critical, batch processing, experiments
```

### Cost Per Use Case

| Task                  | Cheapest         | Cost  | Speed |
| --------------------- | ---------------- | ----- | ----- |
| Analyze SQL           | Sofia (haiku)    | $0.50 | 8s    |
| Write tests           | Tatiane (haiku)  | $0.50 | 10s   |
| Review code           | Matheus (sonnet) | $1.50 | 15s   |
| Design API            | Carlos (sonnet)  | $2.00 | 20s   |
| Architecture decision | Rodrigo (opus)   | $5.00 | 40s   |
| Strategic decision    | Elena (opus)     | $5.00 | 45s   |

---

## âœ… QUALITY CHECKLIST

### Before Pinging an Agent

- [ ] **Question is clear** (specific, not vague)
- [ ] **Context provided** (code, metrics, logs when relevant)
- [ ] **Goal stated** (what success looks like)
- [ ] **Right agent selected** (matching expertise)
- [ ] **No personal data** (no credentials, API keys, etc)
- [ ] **Reasonable timeline** (not "urgent" unless genuinely is)

### After Getting Response

- [ ] **Answer makes sense** (not contradictory)
- [ ] **You understand it** (if not, ask for clarification)
- [ ] **Next steps are clear** (what to do with answer)
- [ ] **Can implement** (not too vague)

### If Response is Wrong

- [ ] **Provide feedback** (tell agent what was wrong)
- [ ] **Give more context** (agent might have misunderstood)
- [ ] **Ask differently** (rephrase if first try didn't work)
- [ ] **Escalate if needed** (if specialist can't help, go to architect)

---

## ğŸ“ LEARNING PATHS

### For New Engineers

**Week 1**: Work with Sofia (data) + Tatiane (testing)

- Understand metrics, test philosophy
- Build confidence

**Week 2**: Pair with Miguel/Beatriz on features

- Learn code patterns, style

**Week 3**: Work with Matheus (Tech Lead) on reviews

- Learn code quality standards

**Week 4**: Feature work with Carlos/Aninha

- Learn architectural patterns

### For New Managers

**Week 1**: Work with Product Manager (Larissa)

- Learn product discovery, sprint planning

**Week 2**: Pair with Engineering Manager (Diego)

- Learn team coordination, blockers

**Week 3**: Work with Tech Lead (Matheus)

- Learn code quality, technical mentoring

**Week 4**: Architecture review with Architects

- Learn technical decision-making

---

## ğŸš¨ COMMON MISTAKES

### âŒ "I pinged the wrong agent"

**Prevention**:

- Use decision trees above
- Check agent expertise (read AGENT_CAPABILITIES_ADVANCED_REFERENCE.md)
- Ask for help: "Who should I ask about [topic]?"

**Recovery**:

- Ping correct agent
- Reference previous conversation

### âŒ "Question too vague"

**Prevention**:

- Provide context (code, metrics, logs)
- State goal clearly ("I want to make this 2x faster")
- Specify constraints ("within 1 hour", "without breaking API")

**Recovery**:

- Clarify: "More specific: [details]"
- Agent will ask for clarification if needed

### âŒ "Waiting for answer took too long"

**Prevention**:

- Parallel: Ping multiple agents simultaneously
- Use Fast agents for quick tasks
- Batch similar requests

**Recovery**:

- Check agent status: `openclaw agents list`
- Escalate to Engineering Manager if blocked

### âŒ "Cost higher than expected"

**Prevention**:

- Use appropriate agent tier (FAST for execution)
- Batch requests (fewer roundtrips)
- Use fallback models for non-critical work

**Recovery**:

- Enable cost tracking: `openclaw session status`
- Adjust model assignment for future

---

## ğŸ“Š SUCCESS METRICS

### Good Collaboration

- âœ… Responses in < 30 seconds
- âœ… First response solves problem (no back-and-forth)
- âœ… Cost is reasonable (<$5 per task)
- âœ… Quality is high (rare follow-ups)
- âœ… Team is productive (short cycle times)

### Signs of Problems

- ğŸ”´ Responses take >60 seconds (agent overloaded)
- ğŸ”´ Multiple clarifications needed (question too vague)
- ğŸ”´ Cost is >$10 per task (using expensive agent unnecessarily)
- ğŸ”´ Quality is poor (wrong answer, irrelevant)
- ğŸ”´ Frequent errors (agent confused)

---

## ğŸ”„ FEEDBACK LOOP

### How Agents Learn (via feedback)

If agent answer was wrong:

```
Good: "That's wrong because [explanation]. The correct answer is [X]."
Bad: "This doesn't work."
```

Agents use feedback to improve future responses. Specific feedback is more valuable.

---

## ğŸ“ ESCALATION CONTACTS

| Problem               | Contact             | Response Time |
| --------------------- | ------------------- | ------------- |
| Agent unresponsive    | Engineering Manager | <10 min       |
| Rate limiting         | DevOps Engineer     | <5 min        |
| Cost too high         | VP Engineering      | <15 min       |
| Architecture question | CTO                 | <10 min       |
| Blocked on decision   | CEO                 | <15 min       |
| Security concern      | CISO                | Immediate     |

---

## ğŸ¬ QUICK REFERENCE: "WHAT TO SAY"

### Good Requests

```
"Carlos, design API for user registration:
  - Email + password input
  - JWT token response
  - Should work with existing DB schema
  - Error cases: invalid email, weak password"

"Sofia, analyze Q1 metrics:
  - Feature adoption rate by cohort
  - Performance trends (API latency, DB query time)
  - User retention by feature"

"Tatiane, write tests for [feature]:
  - 80%+ coverage target
  - Edge cases: [list]
  - Performance: tests should run <100ms"
```

### Bad Requests

```
âŒ "Make API"
âŒ "Fix performance"
âŒ "Write tests"

These lack context and specifics.
```

---

## ğŸ“ˆ NEXT STEPS

1. **Read** `AGENT_SYSTEM_MASTER_AUDIT_20260220.md` (organization + structure)
2. **Read** `AGENT_CAPABILITIES_ADVANCED_REFERENCE_20260220.md` (detailed capabilities)
3. **Use** This playbook for daily work
4. **Provide feedback** if agents don't match descriptions

---

**Last Updated**: 2026-02-20 02:25 PST  
**Version**: 1.0 (Final for Phase 1)  
**Next Review**: After Phase 2 (May 31)

_You now have everything you need to work effectively with 100 agents._
